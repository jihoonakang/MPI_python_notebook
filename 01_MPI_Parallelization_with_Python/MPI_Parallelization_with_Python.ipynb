{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Performance Computing & AI - 2024.10.05\n",
    "\n",
    "파이선 MPI 병렬프로그래밍: MPI 기초 및 예제 실습\n",
    "===================================================\n",
    "\n",
    "\n",
    "### 한국과학기술정보연구원 강지훈\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 분할 (영역 분할)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **각각의 코어 (또는 프로세스)는 데이터를 분할하여 소유** \n",
    "  \n",
    "  - 각 계산 코어는 동기/비동기 통신을 이용하여 필요한 데이터를 주고 받음.\n",
    "  - 분할된 데이터에서 계산을 수행하므로 계산은 자동적으로 병렬화됨.\n",
    "\n",
    "    <img src = \"figs/fig_1.jpg\" height=\"200\">\n",
    "\n",
    "\n",
    "* **MPI 병렬화에 적합**\n",
    "* **프로그래머가 모든 계산과 통신을 신경써야 함: 복잡하지만 유연** \n",
    "  - 데이터 분할, 작업 할당, 데이터 통신, 동기화 등\n",
    "* **클러스터 시스템에 적합한 기본적인 병렬화 방식**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI (Message Passing Interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"figs/fig_2.jpg\" height=\"250\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **프로세스(Process)와 프로세서(Processor)** \n",
    "  \n",
    "  - MPI는 프로세스 기준으로 작업할당 \n",
    "  - 프로세서 대 프로세스 = 일대일(1:1) 또는 일대다(1:n)\n",
    "  \n",
    "\n",
    "- **메시지 (= 데이터 + 봉투(Envelope))**\n",
    "\n",
    "  - 어떤 프로세스가 보내는가 \n",
    "  - 어디에 있는 데이터를 보내는가 \n",
    "  - 어떤 데이터를 보내는가 얼마나 보내는가 \n",
    "  - 어떤 프로세스가 받는가 \n",
    "  - 어디에 저장할 것인가 \n",
    "  - 얼마나 받을 준비를 해야 하는가\n",
    "\n",
    "- **꼬리표(tag)** \n",
    "  \n",
    "  - 메시지 매칭과 구분에 이용 \n",
    "  - 순서대로 메시지 도착을 처리할 수 있음 \n",
    "\n",
    "\n",
    "- **커뮤니케이터(Communicator)** \n",
    "\n",
    "  - 서로간에 통신이 허용되는 프로세스들의 집합 \n",
    "  - 기본적으로 만들어지는 커뮤니케이터: MPI.COMM_WORLD\n",
    "\n",
    "\n",
    "- **랭크(Rank)** \n",
    "\n",
    "  - 동일한 커뮤니케이터 내의 프로세스들을 식별하기 위한 식별자\n",
    "  - **0부터 시작**\n",
    "\n",
    "\n",
    "- **점대점 통신(Point to Point Communication)** \n",
    "  \n",
    "  - 두 개 프로세스 사이의 통신 \n",
    "  - 하나의 송신 프로세스에 하나의 수신 프로세스가 대응\n",
    "\n",
    "\n",
    "- **집합통신(Collective Communication)** \n",
    "\n",
    "  - 동시에 여러 개의 프로세스가 통신에 참여 \n",
    "  - 일대다, 다대일, 다대다 대응 가능 \n",
    "  - 여러 번의 점대점 통신 사용을 하나의 집합통신으로 대체 \n",
    "    - 오류의 가능성이 적다. \n",
    "    - 최적화 되어 일반적으로 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"MPI for Python (mpi4py)\" 는 MPI 표준 라이브러리에 대한 파이선 바인딩 인터페이스를 제공\n",
    "  \n",
    "- 파이선 프로그램들이 워크스테이션, 클러스터, 슈퍼컴퓨터 등 대규모 시스템에서 병렬 프로세서를 이용할 수 있도록 지원\n",
    "\n",
    "- MPI 표준을 따라 구현되며 MPI-2 C++ 바인딩과 유사한 객체지향 인터페이스를 제공\n",
    "\n",
    "- 파이선 피클 객채를 이용한 손쉬운 통신 가능 \n",
    "\n",
    "  - 점대점 통신 (send & receive)\n",
    "  - 집합 통신 (broadcast, scatter & gather, reductions)\n",
    "\n",
    "- 파이선 버퍼 인터페이스를 갖는 객체 (Numpy 처럼 bytes, string, array 객체)를 이용한 빠른 통신 가능\n",
    "\n",
    "  - 점대점 통신 (blocking/nonbloking/persistent send & receive)\n",
    "  - 집합 통신 (broadcast, block/vector scatter & gather, reductions)\n",
    "\n",
    "- 프로세스 그룹과 커뮤니케이터 지원\n",
    "\n",
    "  - Intra/inter 커뮤니케이터의 생성\n",
    "  - 카테시안 & 그래프 토폴로지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 병렬 파일 IO\n",
    "\n",
    "  - 읽기 & 쓰기\n",
    "  - blocking/non-blocking & collective/noncollective\n",
    "  - individual/shared file pointers & explicit offset\n",
    "\n",
    "\n",
    "- 동적 프로세스 관리\n",
    "\n",
    "  - spawn & spawn multiple\n",
    "  - accept/connect\n",
    "  - name publishing & lookup\n",
    "\n",
    "\n",
    "- 일방향 통신 (One-sided operations)\n",
    "\n",
    "  - remote memory access (put, get, accumulate)\n",
    "  - passive target syncronization (start/complete & post/wait)\n",
    "  - active target syncronization (lock & unlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **필수요소**\n",
    "  - Python 3.6 or above\n",
    "  - MPI 라이브러리 (MPICH or Open MPI built with shared/dynamic libraries)\n",
    "\n",
    "- MPI 라이브러리\n",
    "  - Linux\n",
    "    - mpich (https://www.mpich.org/)\n",
    "    - mvapich (https://mvapich.cse.ohio-state.edu/)\n",
    "    - openmpi (https://www.open-mpi.org/)\n",
    "  - Window \n",
    "    - msmpi (https://learn.microsoft.com/ko-kr/message-passing-interface/microsoft-mpi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Miniconda 설치**\n",
    "  - Miniconda 공식 홈페이지: [link](https://docs.anaconda.com/free/miniconda/) \n",
    "  \n",
    "  \n",
    "- **`mpi` 이름으로 `conda`를 이용해서 설치**\n",
    "  ```shell\n",
    "  conda create -n mpi python=3.11 numpy mpi4py matplotlib\n",
    "  ```\n",
    "\n",
    "- **`conda` 활성화**\n",
    "  ```shell\n",
    "  conda activate mpi \n",
    "  ```\n",
    "\n",
    "- **필요한 라이브러리 설치**\n",
    "  ```shell\n",
    "  conda install numpy\n",
    "  ```\n",
    "\n",
    "- **작업 실행**\n",
    "  ```shell\n",
    "  mpiexec -np 3 python [FILE_NAME].py\n",
    "  ```\n",
    "\n",
    "- **`conda` 비활성화**\n",
    "  ```shell\n",
    "  conda deactivate\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - 초기화 / 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **라이브러리 import**\n",
    "\n",
    "  - from mpi4py import MPI \n",
    "\n",
    "- **MPI 초기화와 마무리할 필요 없음**\n",
    "\n",
    "  - Importing mpi4py already triggers MPI_INIT()\n",
    "  - MPI_Finalize() is called when all python processes exit \n",
    "\n",
    "- **필요한 변수 초기화하기**\n",
    "\n",
    "  - `comm = MPI.COMM_WORLD`\n",
    "  - `myrank = comm.Get_rank()`\n",
    "  - `nproc = comm.Get_size()`\n",
    "\n",
    "- **병렬실행 (np: number of processes)**\n",
    "\n",
    "  - `mpirun -np 2 python mycode.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - mpirun / mpiexec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **MPI distributions normally come with an implementation-specific execution utility.** \n",
    "\n",
    "  - Executes program multiple times (SPMD parallel programming) \n",
    "  - Supports multiple nodes \n",
    "  - Integrates with batch queueing systems \n",
    "  - Some implementations use “mpiexec”\n",
    "  \n",
    "\n",
    "- **Examples**\n",
    "\n",
    "  ```shell\n",
    "    mpirun -n 4 python script.py # on a laptop \n",
    "    mpirun --host n01,n02,n03,n04 python script.py \n",
    "    mpirun --hostfile hosts.txt python script.py \n",
    "    mpirun python script.py # with batch queueing system\n",
    "    mpiexec -n 4 python script.py # on a windows OS \n",
    "\n",
    "  ```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - Hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/mpi1.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 3 python ./examples/mpi1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - Using the Comm class to define communicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **size = MPI.COMM_WORLD.Get_size() : 프로세스는 몇 개인가?**\n",
    "  - 커뮤니케이터의 크기 혹은 커뮤니케이터의 프로세스 수를 반환하는 함수 \n",
    "    - MPI.COMM_WORLD.size로 직접 멤버변수에 접근 가능\n",
    "    - COMM_WORLD : 기본 커뮤니케이터\n",
    "  - `mpiexec –np n python MY_PROGRAM` 실행 \n",
    "    - mpiexec은 MY_PROGRAM을 n개의 프로세스로 병렬 실행\n",
    "    - 기본 커뮤니케이터인 MPI.COMM_WORLD를 생성\n",
    "  - 프로세스 수를 argument로 받아서 MPI.COMM_WORLD 객체의 size란 변수 저장\n",
    "- **rank = MPI.COMM_WORLD.Get_rank() : 나는 몇 번인가?**\n",
    "  - 커뮤니케이터에서 할당된 각 프로세스의 랭크를 반환하는 함수\n",
    "    - MPI.COMM_WORLD.rank로 직접 멤버 변수에 접근 가능\n",
    "  - MPI 프로세스는 커뮤니케이터 생성시 (0 ~ n-1) 사이의 랭크를 부여 받음\n",
    "  - 랭크 값을 이용하여 프로세스별 실행 작업을 달리할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/mpi2.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "print(\"Hello World! from process {0} of {1} \\n\".format(rank, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 3 python ./examples/mpi2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - 점대점통신 1 (Point to point communications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가장 기본이 되는 통신 함수 (“send” and “recv”) $\\leftarrow$ 소문자로 시작\n",
    "\n",
    "  - `comm.send(obj, dest, tag=0)` \n",
    "  - `comm.recv(source=MPI.ANY SOURCE, tag=MPI.ANY TAG, status=None)` \n",
    "  \n",
    "  - `“tag”` can be used as a filter \n",
    "  - `“dest”` must be a rank in communicator \n",
    "  - `“source”` can be a rank or MPI.ANY SOURCE (wild card) \n",
    "  - `“status”` used to retrieve information about recv’d message \n",
    "  - These are blocking operations\n",
    "  - Basic Message Passing Process\n",
    "\n",
    "  <img src = \"figs/fig_3.jpg\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - MPI Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **C**\n",
    "  \n",
    "<img src = \"figs/fig_4.jpg\" height=\"300\">\n",
    "\n",
    "- **Fortran**\n",
    "  \n",
    "<img src = \"figs/fig_5.jpg\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - 점대점통신 1 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/p2p_1.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    msg = \"Hello, world\"\n",
    "    comm.send(msg, dest=1)\n",
    "elif rank == 1:\n",
    "    s = comm.recv()\n",
    "    print (\"rank %d: %s\" % (rank, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 2 python ./examples/p2p_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - 점대점통신 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **데이터버퍼와 같은 객체를 이용한 통신 (“Send” and “Recv”) $\\leftarrow$ 대문자로 시작**\n",
    "  \n",
    "  - **버퍼같은 객체 (Buffer-like object)s**\n",
    "    \n",
    "    - `Numpy arrays (class'numpy.ndarray’)`  \n",
    "    - A list or tuple with 2 or 3 elements (or 4 elements for the vector variants)\n",
    "      - 2 elements: [data, MPI.DOUBLE], data (a Numpy array)\n",
    "      - 3 elements: [data, n, MPI.DOUBLE]: data, n (buffer of the first n elements.)\n",
    "  \n",
    "  - `comm.Send(buf, dest, tag=0)` \n",
    "  - `comm.Recv(buf, source=MPI.ANY SOURCE, tag=MPI.ANY TAG, status=None)` \n",
    "    \n",
    "  - **장점**\n",
    "    - 빠른 통신 (MPI C와 같은 수준의 통신 속도)\n",
    "    \n",
    "  - **단점**\n",
    "    \n",
    "    - 데이터형, 데이터주소, 데이터 범위 등 메모리 관련 파라미터들을 보다 명시적으로 표현해야 함\n",
    "    - 수신측의 메모리가 미리 할당되어 있어야 하며, 송수신 데이터 버퍼의 크기에 오류가 없어야 함 (송신버퍼 < 수신버퍼)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/p2p_2.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# master process\n",
    "if rank == 0:\n",
    "    data = np.arange(4.)\n",
    "    # master process sends data to worker processes by\n",
    "    # going through the ranks of all worker processes\n",
    "    for i in range(1, size):\n",
    "        comm.Send(data, dest=i, tag=i)\n",
    "        print('Process {} sent data:'.format(rank), data)\n",
    "\n",
    "# worker processes\n",
    "else:\n",
    "    # initialize the receiving buffer\n",
    "    data = np.zeros(4)\n",
    "    # receive data from master process\n",
    "    comm.Recv(data, source=0, tag=rank)\n",
    "    print('Process {} received data:'.format(rank), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 2 python3 ./examples/p2p_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - 점대점통신 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **동기 통신 (Synchronous communication, Blocking comm.)**\n",
    "  \n",
    "  - 메시지 수신이 완료될 때까지 통신이 종료되지 않음\n",
    "  - 송수신 종료를 상호 확인후 종료\n",
    "\n",
    "- **비동기 통신 (Asynchronous communication, Non-blocking comm.)**\n",
    "\n",
    "  - 메시지 송신 또는 수신 함수 호출과 동시에 리턴\n",
    "  - 이메일과 같음\n",
    "  \n",
    "\n",
    "  <img src = \"figs/fig_6.jpg\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Communiation mode\n",
    "  \n",
    "  <img src = \"figs/fig_7.jpg\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 교착(deadlock) 피하기 (or hung)\n",
    "\n",
    "- **Send & Recv : Blocking communication**\n",
    "  \n",
    "  - **`comm.Send`**\n",
    "    - 송신 프로세스가 데이터를 모두 전송했음을 확인한 후 리턴되어 호출을 종료\n",
    "    - 데이터는 수신 프로세스의 버퍼나 메모리에 전송이 완료되어야 함\n",
    "    - 송신이 끝날 때까지 데이터가 변하지 않음\n",
    "  \n",
    "  - **`comm.Recv`**\n",
    "    - 수신 프로세스의 데이터 수신이 완료된 후 리턴되어 호출을 종료\n",
    "    - 수신이 끝날 때까지 데이터가 변하지 않음\n",
    "\n",
    "   <img src = \"figs/fig_8.jpg\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 교착 없음\n",
    "\n",
    "```python\n",
    "#!--Exchange messages\n",
    "if mpirank == 0 :\n",
    "\tcomm.Send(a, 1, tag1)\n",
    "\tcomm.Recv(b, 1, tag2)\n",
    "elif mpirank == 1 : \n",
    "\tcomm.Recv(a, 0, tag1)\n",
    "\tcomm.Send(b, 0, tag2)\n",
    "```\n",
    "\n",
    "<img src = \"figs/fig_9.jpg\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 무조건 교착\n",
    "\n",
    "```python\n",
    "# !--Exchange messages\n",
    "if mpirank == 0 :\n",
    "\tcomm.Recv(b, 1, tag2)\n",
    "\tcomm.Send(a, 1, tag1)\n",
    "\n",
    "elif mpirank == 1 : \n",
    "\tcomm.Recv(a, 0, tag1)\n",
    "\tcomm.Send(b, 0, tag2)\n",
    "\n",
    "```\n",
    "\n",
    "<img src = \"figs/fig_10.jpg\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 조건부 교착\n",
    "\n",
    "```python\n",
    "# Exchange messages\n",
    "if mpirank == 0 :\n",
    "    comm.Send(a, 1, tag1)\n",
    "    comm.Recv(b, 1, tag2)\n",
    "\n",
    "elif mpirank == 1 : \n",
    "    comm.Send(b, 0, tag2)\n",
    "    comm.Recv(a, 0, tag1)\n",
    "\n",
    "```\n",
    "\n",
    "<img src = \"figs/fig_10_1.png\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/deadlock.py \n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "buf_size = 100\n",
    "\n",
    "a = np.ones(buf_size, dtype = int)\n",
    "b = np.empty(buf_size, dtype = int)\n",
    "if rank == 0 :\n",
    "    comm.Send(a, dest = 1, tag = 11)\n",
    "    comm.Recv(b, source = 1, tag = 55)\n",
    "elif rank ==1 :\n",
    "    comm.Send(a, dest = 0, tag = 55)\n",
    "    comm.Recv(b, source = 0, tag = 11)\n",
    "\n",
    "print(\"Everything okay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 2 python3 ./examples/deadlock.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensuring a Program is Safe (No deadlock) \n",
    "\n",
    "- **`comm.Send` 와 `comm.Ssend`는 기본적으로 동일하다고 생각**\n",
    "  \n",
    "  - `comm.Ssend` 는 항상 동기 통신을 수행하고 `comm.Send`는 버퍼 크기에 따라 비동기 통신처럼 작동하지만 동기 통신을 수행하는 경우도 있음\n",
    "  - `comm.Send` 함수 또한 동기 통신을 수행한다고 가정하고 코드를 작성\n",
    "\n",
    "- **Deadlock 피하기**\n",
    "  - send / receive에 참여하는 프로세스의 호출 순서를 주의깊게 처리\n",
    "  - `comm.Send` 는 동기 통신으로 간주하고 버퍼 통신이 없을 것이라고 간주\n",
    "  - 비통기 통신을 사용\n",
    "  - `comm.Sendrecv`를 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `comm.Sendrecv`\n",
    "\n",
    "- **`Sendrecv(sendbuf, dest, sendtag=0, recvbuf=None, source=ANY_SOURCE, recvtag=ANY_TAG, status=None)`**\n",
    "\n",
    "  - sendbuf (BufSpec) –\n",
    "  - dest (int) –\n",
    "  - sendtag (int) –\n",
    "  - recvbuf (BufSpec) –\n",
    "  - source (int) –\n",
    "  - recvtag (int) –\n",
    "  - status (Optional[Status]) –"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sendrecv.py: using `comm.Sendrecv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/sendrecv.py \n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "a = np.zeros(size, dtype = int)\n",
    "b = np.zeros(size, dtype = int)\n",
    "a[rank] = rank + 1\n",
    "inext = rank + 1\n",
    "iprev = rank - 1\n",
    "\n",
    "if rank == 0 :\n",
    "    iprev = size - 1\n",
    "if rank == size - 1 :\n",
    "    inext = 0\n",
    "\n",
    "for i in range(size) :\n",
    "    if rank == i :\n",
    "        print('BEFORE : myrank={0}, A = {1}'.format(rank, a))\n",
    "\n",
    "comm.Sendrecv(a, inext, 77, b, iprev, 77)\n",
    "\n",
    "for i in range(size) :\n",
    "    if rank == i :\n",
    "        print('AFTER  : myrank={0}, B = {1}'.format(rank, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 2 python3 ./examples/sendrecv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - 점대점통신 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-blocking communications\n",
    "\n",
    "- **비동기 통신의 2단계**\n",
    "  - 비동기 송수신 함수 호출 \n",
    "  - `comm.Isend` 또는 `comm.Irecv`\n",
    "  - 통신 완료를 확인하는 함수를 호출\n",
    "  - `comm.Wait` 또는 `comm.Waitall`\n",
    "\n",
    "- **특징**\n",
    "  - 함수 호출 직후 리턴되어 Deadlock을 무조건적으로 피할 수 있음\n",
    "  - 메시지 송수신의 안전성을 보장할 수 없음 (Send & forget)\n",
    "  - 송수신 함수와 통신 완료 확인 함수 사이에 메시지와 관계 없는 작업을 추가할 수 있음\n",
    "\n",
    "- **`request = comm.Isend(… )`**\n",
    "\n",
    "  - request: request handle\n",
    "\n",
    "\n",
    "- **`request = comm.Irecv(…)`**\n",
    "\n",
    "  - request: request handle\n",
    "\n",
    "\n",
    "- `request.Wait(status=None)`\n",
    "\n",
    "\n",
    "- `MPI.Request.Waitall(requests, statuses=None)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### isendrecv.py: Example of Non-blocking communications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/isendrecv.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank() # myrank = comm.rank\n",
    "\n",
    "data  = np.zeros(100, dtype = float)\n",
    "value = np.zeros(100, dtype = float)\n",
    "req_list = []\n",
    "\n",
    "if rank == 0 :\n",
    "    for i in range(100) :\n",
    "        data[i] = i * 100\n",
    "        req_send = comm.Isend(data[i:i+1], dest = 1, tag = i)\n",
    "        req_list.append(req_send)\n",
    "elif rank == 1 :\n",
    "    for i in range(100) :\n",
    "        req_recv = comm.Irecv(value[i:i+1] , source = 0, tag = i)\n",
    "        req_list.append(req_recv)\n",
    "\n",
    "MPI.Request.Waitall(req_list)\n",
    "\n",
    "if rank == 0 :\n",
    "    print(\"data[99] = {0}\\n\".format(data[99]))\n",
    "\n",
    "if rank == 1 :\n",
    "    print(\"value[99] = {0}\\n\".format(value[99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 2 python ./examples/isendrecv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**동기 통신과 비동기 통신의 차이**\n",
    "  - `comm.Send`\n",
    "    - 통신이 종료될 때까지 함수 호출 상태로 대기 및 통신 종료 확인 후 진행되어 통신 데이터는 안전하게 전달됨.\n",
    "    - 버그 등에 의해 통신이 완료되지 않을 경우 프로그램 실행이 멈춘 상태로 있음 (deadlock)\n",
    "  - `comm.Isend`\n",
    "    - 함수를 호출하자마자 리턴되어 진행되며, 통신은 background 작업으로 실행됨\n",
    "    - 데드락이 발생하지 않지만 통신이 어느 시점에 완료되는지 알 수 없으며 데이터가 덮어써질 위험이 있음.\n",
    "    - 사용자가 반드시 comm.Wait함수의 호출로 통신이 완료될 때까지 대기하도록 해야함\n",
    "  - `comm.Wait`\n",
    "    - Blocking 함수로 해당 request를 호출한 통신 함수가 완전히 통신을 종료할 때까지 대기\n",
    "    - 통신 데이터는 Wait 함수 호출 이후에 안전하게 사용할 수 있음 \n",
    "\n",
    "**프로그래머가 동기화 시점을 결정해야 함**\n",
    "  - Send/Recv에 관계없이 통신 데이터가 필요한 곳 이전에 `comm.Wait`를 이용하여 직접 동기화\n",
    "  - `comm.Wait` 는 블로킹 함수이므로 프로그램의 흐름을 제어할 수 있음\n",
    "\n",
    "**비동기 통신의 유용성**\n",
    "  - 비동기 통신 함수와 Wait함수 사이에 통신 데이터와 관계 없는 작업을 실행시킬 수 있음\n",
    "  - 통신과 계산을 중첩하여 통신을 background작업으로 처리함으로써 통신 시간을 없앨 수 있음 (통신 은폐)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if rank == 0 :\n",
    "    for i in range(100) :\n",
    "        data[i] = i * 100\n",
    "        req_send = comm.Isend(data[i:i+1], dest = 1, tag = i)\n",
    "        req_list.append(req_send)\n",
    "elif rank == 1 :\n",
    "    for i in range(100) :\n",
    "        req_recv = comm.Irecv(value[i:i+1] , source = 0, tag = i)\n",
    "        req_list.append(req_recv)\n",
    "\n",
    "#  We can put calculation**\n",
    "\n",
    "MPI.Request.Waitall(req_list)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬 general object의 통신\n",
    "\n",
    "- **함수 첫글자가 소문자인 통신함수의 경우 일반적인 파이썬 객체를 주고받을 수 있음**\n",
    "  - Comm.send, Comm.recv, Comm.sendrecv 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/general1.py\n",
    "\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "a = rank\n",
    "b = 0\n",
    "\n",
    "inext = rank + 1\n",
    "iprev = rank - 1\n",
    "\n",
    "if rank == 0 :\n",
    "    iprev = size - 1\n",
    "if rank == size - 1 :\n",
    "    inext = 0\n",
    "for i in range(size) :\n",
    "    if rank == i :\n",
    "        print('BEFORE : myrank={0}, A = {1}'.format(rank, a))\n",
    "b = comm.sendrecv(a, inext, 77, None, iprev, 77)\n",
    "for i in range(size) :\n",
    "    if rank == i :\n",
    "        print('AFTER : myrank={0}, B = {1}'.format(rank, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 2 python ./examples/general1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/general2.py\n",
    "\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank() \n",
    "\n",
    "data = []\n",
    "value = []\n",
    "req_list = []\n",
    "if rank == 0 :\n",
    "    for i in range(100) :\n",
    "        data.append(i * 100)\n",
    "        req_send = comm.isend(data[i], dest = 1, tag = i)\n",
    "        req_list.append(req_send)\n",
    "elif rank == 1 :\n",
    "    for i in range(100) :\n",
    "        req_recv = comm.irecv( source = 0, tag = i)\n",
    "        req_list.append(req_recv)\n",
    "value = MPI.Request.waitall(req_list)\n",
    "\n",
    "if rank == 0 :\n",
    "    print(\"data[99] = {0}\\n\".format(data[99]))\n",
    "if rank == 1 :\n",
    "    print(\"value[99] = {0}\\n\".format(value[99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 2 python ./examples/general2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제 1 - 몬테카를로를 이용한 PI 계산\n",
    "\n",
    "  <img src = \"figs/fig_10_2.png\" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#순차코드\n",
    "import numpy as np\n",
    "\n",
    "SCOPE = 1000000\n",
    "count = 0\n",
    "\n",
    "for i in range(SCOPE) :\n",
    "    x = np.random.rand()\n",
    "    y = np.random.rand()\n",
    "    z = (x*x + y*y)**(0.5)\n",
    "    if z < 1 :\n",
    "       count += 1\n",
    "\n",
    "print(\"Count = %d, Pi = %f\"%(count,count/SCOPE*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/pi1.py\n",
    "#병렬 코드\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "SCOPE = 1000000\n",
    "np.random.seed(rank)\n",
    "\n",
    "mycount = 0\n",
    "for i in range(int(SCOPE/size)) :\n",
    "    x = np.random.rand()\n",
    "    y = np.random.rand()\n",
    "    z = (x*x + y*y)**(0.5)\n",
    "    if z < 1 :\n",
    "        mycount += 1\n",
    "\n",
    "if rank == 0 :\n",
    "    for i in range (1, size) :\n",
    "        other_count = comm.recv(source=i, tag=10)\n",
    "        mycount = mycount + other_count\n",
    "else :\n",
    "    comm.send(mycount, dest=0, tag=10)\n",
    "if rank == 0 :\n",
    "    print('Rank : %d, Count = %d, Pi = %f'%(rank,mycount,mycount/SCOPE*4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/pi1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - 집합통신 (Collective communicator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 같은 커뮤니케이터 안에 있는 프로세스들이 한 번에 참여하는 통신\n",
    "- 점대점 통신에 기반하지만 효율적인 알고리즘으로 수행할 수 있도록 API를 제공\n",
    "- 점대점 통신에 비해 사용이 편하고 빠른 성능을 냄\n",
    "\n",
    "- **특징**\n",
    "\n",
    "  - 커뮤니케이터에 있는 모든 프로세스들이 다같이 호출해야 함\n",
    "  - 불필요한 Envelop 정보가 있음 : 메시지 tag이 없음. 통신 형태에 따라 송신/수신 프로세스가 없을 수 있음\n",
    "    \n",
    "\n",
    "  <img src = \"figs/fig_11.jpg\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `comm.bcast(sendobj, root=0)`\n",
    "  <img src = \"figs/fig_12.jpg\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/bcast.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a': 1, 'b': 2, 'c': 3}\n",
    "else:\n",
    "    data = None \n",
    "    \n",
    "data = comm.bcast(data, root=0)\n",
    "print('rank', rank, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python3 ./examples/bcast.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `comm.Bcast(sendobj, root=0)`\n",
    "- Numpy array 등 buffer interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/bcast2.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "\n",
    "rank = comm.Get_rank()\n",
    "ROOT = 0\n",
    "buf = np.zeros(4, dtype = int)\n",
    "buf2 = np.zeros(4, dtype = int)\n",
    "if rank == ROOT :\n",
    "    buf = np.array([5, 6, 7, 8])\n",
    "if rank == (size - 1) :\n",
    "    buf2 = np.array([50, 60, 70, 80])\n",
    "print('Before : rank = {0}, buf = {1}'.format(rank, buf))\n",
    "comm.Bcast(buf, ROOT)\n",
    "comm.Bcast(buf2, size-1)\n",
    "\n",
    "print('After : rank = {0}, buf = {1}'.format(rank, buf))\n",
    "print('After : rank = {0}, buf2 = {1}'.format(rank, buf2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python3 ./examples/bcast2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `recvobj = comm.gather(sendobj, root=0)`\n",
    "  <img src = \"figs/fig_15.jpg\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/gather1.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "recvbuf = comm.gather(rank)\n",
    "if rank == 0:\n",
    "    print('Gathered array: {}'.format(recvbuf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/gather1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `comm.Gather(sendbuf, recvbuf, root=0)`\n",
    "  <img src = \"figs/fig_16.jpg\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/gather2.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size ()\n",
    "\n",
    "local_array = [rank] * 4\n",
    "print(\"rank: {}, local_array: {}\".format(rank, local_array))\n",
    "sendbuf = np.array(local_array)\n",
    "\n",
    "recvbuf = None\n",
    "if rank == 0:\n",
    "    recvbuf = np.empty(size*4, dtype=int)\n",
    "\n",
    "comm.Gather(sendbuf=sendbuf, recvbuf=recvbuf)\n",
    "if rank == 0:\n",
    "    print(\"Gathered array: {}\".format(recvbuf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/gather2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `recvobj = comm.scatter(sendobj, root=0)`\n",
    "  <img src = \"figs/fig_18.jpg\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/scatter.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = [(i+1)**2 for i in range(size)]\n",
    "else:\n",
    "    data = None\n",
    "\n",
    "data = comm.scatter(data, root=0)\n",
    "assert data == (rank+1)**2\n",
    "print(\"rank: {}, scattered: {}\".format(rank, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/scatter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `comm.Scatter(sendbuf, recvbuf, root=0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/scatter2.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "isend = np.zeros(size, dtype = int)\n",
    "irecv = np.empty(1, dtype = int)\n",
    "if rank == 0 :\n",
    "    isend = np.arange(0, size, dtype = int)\n",
    "print('sbuf : rank = {0}, irecv = {1}'.format(rank, isend))\n",
    "comm.Scatter(isend, irecv, 0)\n",
    "print('rbuf : rank = {0}, irecv = {1}'.format(rank, irecv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/scatter2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `recvobj = comm.allgather(sendobj)`\n",
    "  <img src = \"figs/fig_20.jpg\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/allgather.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "data = rank**2\n",
    "data = comm.allgather(data)\n",
    "\n",
    "print(\"On rank\", rank,\"data =\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/allgather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `comm.Allgather(sendobj, recvobj)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/allgather2.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "isend = np.array([rank + 1])\n",
    "irecv = np.zeros(size, dtype = int)\n",
    "print('rank = {0}, isend = {1}'.format(rank, isend))\n",
    "comm.Allgather(isend, irecv)\n",
    "print('rank = {0}, irecv = {1}'.format(rank, irecv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/allgather2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `recvobj = comm.alltoall(sendobj)`\n",
    "  <img src = \"figs/fig_21.jpg\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/alltoall.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "rank = comm.Get_rank()\n",
    "data = range(comm.Get_size())\n",
    "print(\"On rank\", rank,\"original data =\", list(data))\n",
    "data = comm.alltoall(data)\n",
    "\n",
    "print(\"On rank\", rank,\"transposed data =\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/alltoall.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `comm.Alltoall(sendbuf, recvbuf)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/alltoall2.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "isend = np.arange(1 + size * rank, 1 + size * rank + size, dtype = int)\n",
    "irecv = np.zeros(size, dtype = int)\n",
    "print('Rank({0}) : isend = {1}'.format(rank, isend))\n",
    "\n",
    "comm.Alltoall(isend, irecv)\n",
    "print('Rank({0}) : irecv = {1}'.format(rank, irecv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/alltoall2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `comm.reduce(sendobj, op=MPI.SUM, root=0)`\n",
    "  <img src = \"figs/fig_13.jpg\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/reduce.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank ()\n",
    "size = comm.Get_size ()\n",
    "\n",
    "local = random.randint(2, 5)\n",
    "print(\"rank: {}, local: {}\".format(rank, local))\n",
    "\n",
    "sum = comm.reduce(local, MPI.SUM, root=0)\n",
    "if (rank==0):\n",
    "    print (\"sum: \", sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/reduce.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `comm.Reduce(sendbuf, recvbuf, op, root = 0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/reduce2.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "a = np.zeros(3, dtype = int)\n",
    "ista = rank * 3\n",
    "iend = ista + 3\n",
    "a= np.arange(ista + 1, iend + 1)\n",
    "sum = a.sum()\n",
    "\n",
    "print('Rank({0}) : local_sum = {1}'.format(rank, sum))\n",
    "tsum = np.zeros_like(sum)\n",
    "comm.Reduce(sum, tsum, MPI.SUM, 0)\n",
    "\n",
    "if rank == 0 :\n",
    "    print('Rank({0}) : sum = {1}'.format(rank, tsum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/reduce2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `recvobj = comm.allreduce(sendobj)`\n",
    "  <img src = \"figs/fig_19.jpg\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/allreduce.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "data = rank**2\n",
    "data = comm.allreduce(data)\n",
    "\n",
    "print(\"On rank\", rank,\"data =\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 3 python ./examples/allreduce.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `comm.Allreduce(sendbuf, recvbuf, op)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/allreduce2.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "\n",
    "rank = comm.Get_rank()\n",
    "a = np.zeros(3, dtype = int)\n",
    "ista = rank * 3\n",
    "iend = ista + 3\n",
    "a = np.arange(ista + 1, iend + 1)\n",
    "\n",
    "sum = a.sum()\n",
    "tsum = np.zeros_like(sum)\n",
    "\n",
    "comm.Allreduce(sum, tsum, MPI.SUM)\n",
    "\n",
    "if rank == 1 :\n",
    "    print('Rank({0}) : sum = {1}'.format(rank, tsum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 3 python ./examples/allreduce2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `comm.scan(sendobj, op=MPI.SUM, root=0)`\n",
    "  <img src = \"figs/fig_14.jpg\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/scan.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank ()\n",
    "size = comm.Get_size ()\n",
    "\n",
    "local = random.randint(2, 5)\n",
    "print(\"rank: {}, local: {}\".format(rank, local))\n",
    "\n",
    "scan = comm.scan(local, MPI.SUM)\n",
    "print (\"rank:\", rank, \"sum: \", scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/scan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `comm.Scan(sendbuf, recvbuf, op)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/scan2.py\n",
    "\n",
    "from mpi4py import MPI \n",
    "import numpy as np \n",
    "import random\n",
    "comm = MPI.COMM_WORLD\n",
    " \n",
    "rank = comm.Get_rank () \n",
    "size = comm.Get_size ()\n",
    "\n",
    "local = random.randint(2, 5)\n",
    "\n",
    "print(\"rank: {}, local: {}\".format(rank, local))\n",
    "\n",
    "scan = comm.scan(local, MPI.SUM) \n",
    "\n",
    "print (\"rank:\", rank, \"sum: \", scan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/scan2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `comm.Reduce_scatter(sendbuf, recvbuf, rcnts, op)`\n",
    "  <img src = \"figs/fig_16_2.png\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/rdc_sct.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "sendbuf = np.array([1, 2, 3], dtype = int)\n",
    "recvbuf = np.zeros(1, dtype = int)\n",
    "RECVBUF = sendbuf * 2\n",
    "\n",
    "comm.Reduce_scatter(sendbuf, recvbuf, None, MPI.SUM)\n",
    "print('Rank({0}) : recvbuf = {1}'.format(rank, recvbuf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 3 python ./examples/rdc_sct.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 집합 통신의 확장\n",
    "- Variant\n",
    "  - 프로세스별로 송수신 크기를 다르게 할 수 있음\n",
    "  - Gatherv, Scatterv, Allgatherv, Alltoallv\n",
    "\n",
    "- 송신버퍼 대치\n",
    "  - 수신버퍼 자리에 MPI.IN_PLACE를 사용하면 송신버퍼가 수신데이터로 대치됨\n",
    "\n",
    "- Maxloc\n",
    "  - Reduce중 maximum의 위치를 반환\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `comm.Gatherv(sendbuf, recvbuf=(recvbuf, sendcounts), root=0)`\n",
    "  <img src = \"figs/fig_17.jpg\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/gatherv.py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "import random\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "local_array = [rank] * random.randint(2, 5)\n",
    "print(\"rank: {}, local_array: {}\".format(rank, local_array))\n",
    "sendbuf = np.array(local_array)\n",
    "sendcounts = np.array(comm.gather(len(sendbuf), 0))\n",
    "recvbuf = None\n",
    "if rank == 0:\n",
    "    print(\"sendcounts: {}, total: {}\".format(sendcounts, sum(sendcounts)))\n",
    "    recvbuf = np.empty(sum(sendcounts), dtype=int)\n",
    "comm.Gatherv(sendbuf=sendbuf, recvbuf=(recvbuf, sendcounts), root=0)\n",
    "if rank == 0:\n",
    "    print(\"Gathered array: {}\".format(recvbuf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/gatherv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <img src = \"figs/fig_22.jpg\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - Loop parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Loop is frequently found in computer algorithm.** \n",
    "\n",
    "  <img src = \"figs/fig_23.jpg\" height=\"80\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Loop parallelizatioin is essential for most computational problem.**\n",
    "\n",
    "  <img src = \"figs/fig_24.jpg\" height=\"80\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - Block distribution - para_range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  <img src = \"figs/fig_25.jpg\" height=\"80\">\n",
    "\n",
    "- **Suppose when you divide `n` by `p`, the quotient is `q` and the remainder is `r`.**\n",
    " \n",
    "  - $n = p\\times q + r$ \n",
    "\n",
    "- **Proccess `0, ..., r-1` are assigned `q+1` iterations each. The other processes are assigned `q` iterations.**\n",
    "\n",
    "  - $n = r(q+1) + (p-r)q$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "## para_range functions \n",
    "\n",
    "def para_range(n1, n2, size, rank):\n",
    "    iwork = divmod((n2 - n1 + 1), size)\n",
    "    ista = rank * iwork[0] + n1 + min(rank, iwork[1])\n",
    "    iend = ista + iwork[0] - 1\n",
    "    if iwork[1] > rank :\n",
    "        iend = iend + 1\n",
    "\n",
    "    return ista, iend\n",
    "\n",
    "def para_range2(n1, n2, size, rank):\n",
    "  N=(n2-n1+1)//size+((n2-n1+1)%size > rank)\n",
    "  end=comm.scan(N)\n",
    "  start=end-N\n",
    "\n",
    "  return n1+start-1, n1+end-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - example1 (`map.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/map.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank ()\n",
    "size = comm.Get_size ()\n",
    "\n",
    "x = range(20)\n",
    "m = int(math.ceil(float(len(x)) / size))\n",
    "x_chunk = x[rank*m:(rank+1)*m]\n",
    "r_chunk = map(math.sqrt, x_chunk)\n",
    "r = comm.reduce(list(r_chunk))\n",
    "if (rank==0):\n",
    "    print (r)\n",
    "\n",
    "#serial => print(list(map(math.sqrt, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python ./examples/map.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - example2 (`pi.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <img src = \"figs/fig_26.jpg\" height=\"350\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/pi_serial.py\n",
    "\n",
    "import math\n",
    "from time import perf_counter\n",
    "#starting value of x\n",
    "x=-1\n",
    "dx=0.0000001\n",
    "start_time = perf_counter()\n",
    "iters=int((1-(-1))/dx)\n",
    "#the sum of all the areas - start at 0\n",
    "A=0.\n",
    "for i in range(iters):\n",
    "    A=A+math.sqrt(1-x**2)*dx\n",
    "    x=x+dx\n",
    "tpi=2*A\n",
    "error = abs(tpi - math.pi)\n",
    "end_time = perf_counter()\n",
    "print (\"pi is approximately %.16f, \"\n",
    "    \"error is %.16f\" % (tpi, error))\n",
    "print('Elapsed wall clock time = %g seconds.' % (end_time-start_time) )\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ./examples/pi_serial.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/pi_mpi.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "from time import perf_counter\n",
    "import math\n",
    "comm=MPI.COMM_WORLD\n",
    "size=comm.Get_size()\n",
    "rank=comm.Get_rank()\n",
    "x=-1\n",
    "dx=0.0000001\n",
    "start_time = perf_counter()\n",
    "iters=int((1-(-1))/dx)\n",
    "N=iters//size+(iters % size > rank)\n",
    "start=comm.scan(N)-N\n",
    "A=0.\n",
    "x=x+dx*start\n",
    "for i in range(N):\n",
    "    A=A+math.sqrt(1-x**2)*dx\n",
    "    x=x+dx\n",
    "##\n",
    "A = comm.reduce(A, op=MPI.SUM, root=0) \n",
    "##\n",
    "\n",
    "end_time = perf_counter()\n",
    "if rank==0:\n",
    "    tpi=2*A\n",
    "    error = abs(tpi - math.pi)\n",
    "    print (\"pi is approximately %.16f, \"\n",
    "        \"error is %.16f\" % (tpi, error))\n",
    "    print('Elapsed wall clock time = %g seconds.' % (end_time-start_time) )\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python3 ./examples/pi_mpi.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - example3: vector + vector (`vecadd.py`: serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/vecadd.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np \n",
    "\n",
    "N = 10000000\n",
    "\n",
    "# initialize a\n",
    "start_time = MPI.Wtime()\n",
    "a = np.ones( N )\n",
    "end_time = MPI.Wtime()\n",
    "print(\"Initialize a time: \" + str(end_time-start_time))\n",
    "\n",
    "# initialize b\n",
    "start_time = MPI.Wtime()\n",
    "b = np.zeros( N )\n",
    "for i in range( N ):\n",
    "    b[i] = 1.0 + i\n",
    "end_time = MPI.Wtime()\n",
    "print(\"Initialize b time: \" + str(end_time-start_time))\n",
    "\n",
    "# add the two arrays\n",
    "start_time = MPI.Wtime()\n",
    "for i in range( N ):\n",
    "    a[i] = a[i] + b[i]\n",
    "end_time = MPI.Wtime()\n",
    "print(\"Add arrays time: \" + str(end_time-start_time))\n",
    "\n",
    "# average the result\n",
    "start_time = MPI.Wtime()\n",
    "sum = 0.0\n",
    "\n",
    "for i in range( N ):\n",
    "    sum += a[i]\n",
    "average = sum / N\n",
    "end_time = MPI.Wtime()\n",
    "\n",
    "print(\"Average result time: \" + str(end_time-start_time))\n",
    "print(\"Average: \" + str(average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ./examples/vecadd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - example3: (`vecadd.py`: 점대점통신 함수를 이용한 병렬화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/vecadd_mpi.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np \n",
    "\n",
    "## fix me example3.1\n",
    "## def para_range\n",
    "def para_range(n1, n2, size, rank) :\n",
    "    iwork = divmod((n2 - n1 + 1), size)\n",
    "    ista = rank * iwork[0] + n1 + min(rank, iwork[1])\n",
    "    iend = ista + iwork[0] - 1\n",
    "    if iwork[1] > rank :\n",
    "        iend = iend + 1\n",
    "    return ista, iend\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "N = 10000000\n",
    "\n",
    "## fix me example3.1\n",
    "# mpi process(rank) 별로 workload 새로 구하기\n",
    "(my_start, my_end) = para_range(0, N-1, size, rank)\n",
    "workload = my_end - my_start + 1\n",
    "\n",
    "# initialize a\n",
    "start_time = MPI.Wtime()\n",
    "a = np.ones(workload)\n",
    "#FIX me example3.2 \n",
    "end_time = MPI.Wtime()\n",
    "if rank == 0:\n",
    "    print(\"Initialize a time: \" + str(end_time-start_time))\n",
    "\n",
    "# initialize b\n",
    "start_time = MPI.Wtime()\n",
    "b = np.ones(workload) #FIX me example3.2 \n",
    "for i in range(workload): #FIX me example3.3\n",
    "    b[i] = 1.0 + i + my_start\n",
    "end_time = MPI.Wtime()\n",
    "if rank == 0:\n",
    "    print(\"Initialize b time: \" + str(end_time-start_time))\n",
    "\n",
    "# add the two arrays\n",
    "start_time = MPI.Wtime()\n",
    "for i in range(workload): #FIX me example3.3\n",
    "    a[i] = a[i] + b[i]\n",
    "end_time = MPI.Wtime()\n",
    "if rank == 0:\n",
    "    print(\"Add arrays time: \" + str(end_time-start_time))\n",
    "\n",
    "# average the result\n",
    "start_time = MPI.Wtime()\n",
    "sum = 0.0\n",
    "\n",
    "for i in range(workload): #FIX me example3.4\n",
    "    sum += a[i]\n",
    "#average = sum / N #FIX me example3.5\n",
    "if rank == 0:\n",
    "    world_sum = sum\n",
    "    for i in range( 1, size ):\n",
    "        sum_np = np.empty( 1 )\n",
    "        comm.Recv( [sum_np, MPI.DOUBLE], source=i, tag=77 )\n",
    "        world_sum += sum_np[0]\n",
    "    average = world_sum / N\n",
    "else:\n",
    "    sum_np = np.array( [sum] )\n",
    "    comm.Send( [sum_np, MPI.DOUBLE], dest=0, tag=77 )\n",
    "\n",
    "end_time = MPI.Wtime()\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"Average result time: \" + str(end_time-start_time))\n",
    "    print(\"Average: \" + str(average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **example3.1: para_range를 이용해서 각 mpi process(rank) 별로 workload 새로 구하기** \n",
    "\n",
    "    ```python\n",
    "        def para_range(n1, n2, size, rank) :\n",
    "            iwork = divmod((n2 - n1 + 1), size)\n",
    "            ista = rank * iwork[0] + n1 + min(rank, iwork[1])\n",
    "            iend = ista + iwork[0] - 1\n",
    "            if iwork[1] > rank :\n",
    "                iend = iend + 1\n",
    "            return ista, iend\n",
    "    ```\n",
    "\n",
    "    ```python\n",
    "        # determine the workload of each rank\n",
    "        (my_start, my_end) = para_range(0, N-1, world_size, my_rank)\n",
    "        workload = my_end - my_start + 1\n",
    "    ```\n",
    "\n",
    "- **example3.2: a, b, vector 메모리 초기화 부분 수정**\n",
    "\n",
    "    `np.ones(N)` $\\rightarrow$ `np.one(workload)`\n",
    "\n",
    "- **example3.3: b vector 초기화 / two vectors 더하기 계산 부분 각각 수정**\n",
    "\n",
    "    `for i in range(N)` $\\rightarrow$ `for i in range(workload)`\n",
    "\n",
    "    `b[i] = 1.0 + i` $\\rightarrow$ `b[i] = 1.0 + (i + my_start)`\n",
    "\n",
    "- **example3.4: average the result 부분 수정**\n",
    "\n",
    "    `for i in range(N)` $\\rightarrow$ `for i in range(workload)`\n",
    "\n",
    "- **example3.5: average 계산부분 수정**\n",
    "\n",
    "    `average = sum / N` $\\rightarrow$ \n",
    "\n",
    "    ```python\n",
    "        if my_rank == 0:\n",
    "            world_sum = sum\n",
    "            for i in range( 1, world_size ):\n",
    "                sum_np = np.empty( 1 )\n",
    "                world_comm.Recv( [sum_np, MPI.DOUBLE], source=i, tag=77 )\n",
    "                world_sum += sum_np[0]\n",
    "            average = world_sum / N\n",
    "        else:\n",
    "            sum_np = np.array( [sum] )\n",
    "            world_comm.Send( [sum_np, MPI.DOUBLE], dest=0, tag=77 )\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python3 ./examples/vecadd_mpi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI4py - example3: (`vecadd.py`: 집합통신 함수를 이용한 병렬화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./examples/vecadd_mpi2.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np \n",
    "\n",
    "## fix me example3.1\n",
    "## def para_range\n",
    "def para_range(n1, n2, size, rank) :\n",
    "    iwork = divmod((n2 - n1 + 1), size)\n",
    "    ista = rank * iwork[0] + n1 + min(rank, iwork[1])\n",
    "    iend = ista + iwork[0] - 1\n",
    "    if iwork[1] > rank :\n",
    "        iend = iend + 1\n",
    "    return ista, iend\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "N = 10000000\n",
    "\n",
    "## fix me example3.1\n",
    "# mpi process(rank) 별로 workload 새로 구하기\n",
    "(my_start, my_end) = para_range(0, N-1, size, rank)\n",
    "workload = my_end - my_start + 1\n",
    "\n",
    "# initialize a\n",
    "start_time = MPI.Wtime()\n",
    "a = np.ones(workload)\n",
    "#FIX me example3.2 \n",
    "end_time = MPI.Wtime()\n",
    "if rank == 0:\n",
    "    print(\"Initialize a time: \" + str(end_time-start_time))\n",
    "\n",
    "# initialize b\n",
    "start_time = MPI.Wtime()\n",
    "b = np.ones(workload) #FIX me example3.2 \n",
    "for i in range(workload): #FIX me example3.3\n",
    "    b[i] = 1.0 + i + my_start\n",
    "end_time = MPI.Wtime()\n",
    "if rank == 0:\n",
    "    print(\"Initialize b time: \" + str(end_time-start_time))\n",
    "\n",
    "# add the two arrays\n",
    "start_time = MPI.Wtime()\n",
    "for i in range(workload): #FIX me example3.3\n",
    "    a[i] = a[i] + b[i]\n",
    "end_time = MPI.Wtime()\n",
    "if rank == 0:\n",
    "    print(\"Add arrays time: \" + str(end_time-start_time))\n",
    "\n",
    "# average the result\n",
    "start_time = MPI.Wtime()\n",
    "sum = 0.0\n",
    "\n",
    "for i in range(workload): #FIX me example3.4\n",
    "    sum += a[i]\n",
    "\n",
    "world_sum = comm.reduce(sum, op = MPI.SUM, root = 0)\n",
    "\n",
    "if rank == 0:\n",
    "    average = world_sum / N\n",
    "\n",
    "end_time = MPI.Wtime()\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"Average result time: \" + str(end_time-start_time))\n",
    "    print(\"Average: \" + str(average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **average 계산부분 수정**\n",
    "     \n",
    "    ```python\n",
    "        if my_rank == 0:\n",
    "            world_sum = sum\n",
    "            for i in range( 1, world_size ):\n",
    "                sum_np = np.empty( 1 )\n",
    "                world_comm.Recv( [sum_np, MPI.DOUBLE], source=i, tag=77 )\n",
    "                world_sum += sum_np[0]\n",
    "            average = world_sum / N\n",
    "        else:\n",
    "            sum_np = np.array( [sum] )\n",
    "            world_comm.Send( [sum_np, MPI.DOUBLE], dest=0, tag=77 )\n",
    "\n",
    "    ```\n",
    "\n",
    "    $\\rightarrow$\n",
    "\n",
    "    ```python\n",
    "        world_sum = comm.reduce(sum, op = MPI.SUM, root = 0)\n",
    "        if rank == 0:\n",
    "            average = world_sum / N\n",
    "    ```\n",
    "    or\n",
    "\n",
    "    ```python\n",
    "        sum = np.array( [sum] )\n",
    "        world_sum = np.zeros( 1 )\n",
    "        world_comm.Reduce( [sum, MPI.DOUBLE], \\\n",
    "            [world_sum, MPI.DOUBLE], op = MPI.SUM, root = 0 )\n",
    "        if rank == 0:\n",
    "            average = world_sum / N\n",
    "    \n",
    "    ```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -np 4 python3 ./examples/vecadd_mpi2.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
